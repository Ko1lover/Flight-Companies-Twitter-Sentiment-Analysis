{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3da7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import emoji\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sqlite3 import Error\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# paths \n",
    "db_name = 'data/dbl.db'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a925290",
   "metadata": {},
   "source": [
    "# Connection to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4827070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to a SQLite database \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        \n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(sqlite3.version)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8994c4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SQLite\n",
      "List of tables\n",
      "\n",
      "[('users',), ('tweets',), ('tweets_geo',), ('replies',), ('retweets',), ('quotes',), ('hashtags',), ('sqlite_sequence',), ('symbols',), ('user_mentions',)]\n",
      "the sqlite connection is closed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "     \n",
    "    # Making a connection between sqlite3\n",
    "    # database and Python Program\n",
    "    sqliteConnection = sqlite3.connect(db_name)\n",
    "     \n",
    "    # If sqlite3 makes a connection with python\n",
    "    # program then it will print \"Connected to SQLite\"\n",
    "    # Otherwise it will show errors\n",
    "    print(\"Connected to SQLite\")\n",
    " \n",
    "    # Getting all tables from sqlite_master\n",
    "    sql_query = \"\"\"SELECT name FROM sqlite_master\n",
    "    WHERE type='table';\"\"\"\n",
    " \n",
    "    # Creating cursor object using connection object\n",
    "    cursor = sqliteConnection.cursor()\n",
    "     \n",
    "    # executing our sql query\n",
    "    cursor.execute(sql_query)\n",
    "    print(\"List of tables\\n\")\n",
    "     \n",
    "    # printing all tables list\n",
    "    print(cursor.fetchall())\n",
    " \n",
    "except sqlite3.Error as error:\n",
    "    print(\"Failed to execute the above query\", error)\n",
    "     \n",
    "finally:\n",
    "   \n",
    "    # Inside Finally Block, If connection is\n",
    "    # open, we need to close it\n",
    "    if sqliteConnection:\n",
    "         \n",
    "        # using close() method, we will close\n",
    "        # the connection\n",
    "        sqliteConnection.close()\n",
    "         \n",
    "        # After closing connection object, we\n",
    "        # will print \"the sqlite connection is\n",
    "        # closed\"\n",
    "        print(\"the sqlite connection is closed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8ef919",
   "metadata": {},
   "source": [
    "# Classifing companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88dc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "avia_companies = {'KLM': 56377143, 'AirFrance': 106062176, 'British_Airways': 18332190, 'AmericanAir': 22536055,\n",
    "                  'Lufthansa': 124476322, 'AirBerlin': 26223583,\n",
    "                  'AirBerlin assist': 2182373406, 'easyJet': 38676903, 'RyanAir': 1542862735, 'SingaporeAir': 253340062,\n",
    "                  'Qantas': 218730857, 'EtihadAirways': 45621423,\n",
    "                  'VirginAtlantic': 20626359}\n",
    "\n",
    "# British Airways, EtihadAirways, VirginAtlantic are private companies, so no tickers of those are available\n",
    "# AirFrance and KLM had a merger at 2004 same with AirBerlin and AirBerlin assist\n",
    "# AirBerlin assist is not listed\n",
    "# Write a regular expression for Air Berlin, RyanAir because they have matching pattern\n",
    "\n",
    "\n",
    "stock = {56377143: ['AF.PA', 'AFLYY', 'AFRAF', 'AFR.F', 'AIRF-U.TI'],  # KLM\n",
    "         106062176: ['AF.PA', 'AFLYY', 'AFRAF', 'AFR.F', 'AIRF-U.TI'],  # AirFrance\n",
    "         22536055: ['AAL', 'A1G.DU', 'AAL.MX', 'AAL.BA'],  # AmericanAir\n",
    "         124476322: ['LHA.DE', 'DLAKY', 'LHA.F', 'DLAKF', 'LHA.SG'],  # Lufthansa\n",
    "         26223583: ['AB1.F', 'AB1.HA', 'AB1.MU', 'AB1.DU', 'AB1.BE', 'AB1.HM'],  # AirBerlin\n",
    "         38676903: ['EZJ.L', 'ESYJY', 'EJT1.DE', 'EJTTF', 'EJT1.HA', 'EJTS.F'],  # EasyJet\n",
    "         1542862735: ['RYAAY', 'RYA.IR', 'RY4C.F', 'RY4C.DE', 'RY4C.BE', 'RY4C.DU'],  # RyanAir\n",
    "         253340062: ['C6L.SI', 'SINGY', 'SINGF', 'SIA1.MU', 'SIA.MU', 'SIA1.HA']}  # SingaporeAir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7949741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "cnx = create_connection(db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a526f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes from the database \n",
    "\n",
    "users = pd.read_sql_query(\"SELECT * FROM users\", cnx)\n",
    "tweets = pd.read_sql_query(\"SELECT * FROM tweets\", cnx)\n",
    "tweets_geo = pd.read_sql_query(\"SELECT * FROM tweets_geo\", cnx)\n",
    "replies = pd.read_sql_query(\"SELECT * FROM replies\", cnx)\n",
    "retweets = pd.read_sql_query(\"SELECT * FROM retweets\", cnx)\n",
    "quotes = pd.read_sql_query(\"SELECT * FROM quotes\", cnx)\n",
    "hashtags = pd.read_sql_query(\"SELECT * FROM hashtags\", cnx)\n",
    "sqlite_sequence = pd.read_sql_query(\"SELECT * FROM sqlite_sequence\", cnx)\n",
    "symbols = pd.read_sql_query(\"SELECT * FROM symbols\", cnx)\n",
    "user_mentions = pd.read_sql_query(\"SELECT * FROM user_mentions\", cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78566b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6094135/6094135 [00:16<00:00, 364293.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert the timestamp in datetime format and then to the string \n",
    "\n",
    "tweets['created_at'] = tweets['timestamp_ms'].progress_apply(lambda x: datetime.fromtimestamp(int(x/1000)))                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26110019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the hour month day and year columns \n",
    "\n",
    "tweets['year'] = tweets.created_at.dt.year\n",
    "tweets['month'] = tweets.created_at.dt.month\n",
    "tweets['day'] = tweets.created_at.dt.day\n",
    "tweets['hour'] = tweets.created_at.dt.hour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42c1b241",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7de001fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6094135/6094135 [00:15<00:00, 390480.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# get text lenghts = word count in the tweet\n",
    "\n",
    "text_len = []\n",
    "for text in tqdm(tweets.text):\n",
    "    tweet_len = len(text.split())\n",
    "    text_len.append(tweet_len)\n",
    "tweets['text_len'] = text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4952be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the functions for cleaning\n",
    "\n",
    "def demojize_emoji(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "def remove_entities(text):\n",
    "    # Getting rid off \\n and \\r\n",
    "    text = text.replace('\\r', r'').replace('\\n', r' ')\n",
    "\n",
    "    text = re.sub(r\"(?:(https?|www)\\://)\\S+|#|:|\\$|@\", r\" \", text)\n",
    "\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def filter_chars(a):\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def remove_mult_spaces(text): ## remove multiple spaces\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "def remove_spam(text):\n",
    "    match = re.search(r'subscribe', text)\n",
    "    if match:\n",
    "        return ''\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "485dbff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6094135/6094135 [08:37<00:00, 11774.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# constructing the list with clean text \n",
    "\n",
    "text_new = []\n",
    "for t in tqdm(tweets.text):\n",
    "    text_new.append(remove_spam(remove_mult_spaces(filter_chars(remove_entities(demojize_emoji(t))))))\n",
    "    \n",
    "tweets['text_clean'] = text_new\n",
    "tweets['text_clean'] = tweets['text_clean'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b352f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6094135/6094135 [00:13<00:00, 438712.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# construction the list with text lenghts \n",
    "\n",
    "text_len = []\n",
    "for text in tqdm(tweets.text_clean):\n",
    "    tweet_len = len(text.split())\n",
    "    text_len.append(tweet_len)\n",
    "tweets['text_clean_len'] = text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5eede89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>created_at</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1131172858951024641</td>\n",
       "      <td>393374091</td>\n",
       "      <td>1558527600406</td>\n",
       "      <td>La ruta de easyJet entre Londres y Menorca tra...</td>\n",
       "      <td>es</td>\n",
       "      <td>original</td>\n",
       "      <td>2019-05-22 14:20:00</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>la ruta de easyjet entre londres y menorca tra...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1131172864147808257</td>\n",
       "      <td>3420691215</td>\n",
       "      <td>1558527601645</td>\n",
       "      <td>@goody_tracy Here’s a list of some of @JonesDa...</td>\n",
       "      <td>en</td>\n",
       "      <td>retweet</td>\n",
       "      <td>2019-05-22 14:20:01</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>goody_tracy heres a list of some of jonesday ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1131172867985485824</td>\n",
       "      <td>394376606</td>\n",
       "      <td>1558527602560</td>\n",
       "      <td>@British_Airways</td>\n",
       "      <td>und</td>\n",
       "      <td>reply</td>\n",
       "      <td>2019-05-22 14:20:02</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>british_airways</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1131172909463027720</td>\n",
       "      <td>36488556</td>\n",
       "      <td>1558527612449</td>\n",
       "      <td>Nice change by @AmericanAir. Bikes now pay sta...</td>\n",
       "      <td>en</td>\n",
       "      <td>retweet</td>\n",
       "      <td>2019-05-22 14:20:12</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>nice change by americanair. bikes now pay stan...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1131172975682605058</td>\n",
       "      <td>14193348</td>\n",
       "      <td>1558527628237</td>\n",
       "      <td>BREAKING:-\\nKLM to fly 3x weekly btw @BLRAirpo...</td>\n",
       "      <td>en</td>\n",
       "      <td>retweet</td>\n",
       "      <td>2019-05-22 14:20:28</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>breaking - klm to fly 3x weekly btw blrairport...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id     user_id   timestamp_ms  \\\n",
       "0  1131172858951024641   393374091  1558527600406   \n",
       "1  1131172864147808257  3420691215  1558527601645   \n",
       "2  1131172867985485824   394376606  1558527602560   \n",
       "3  1131172909463027720    36488556  1558527612449   \n",
       "4  1131172975682605058    14193348  1558527628237   \n",
       "\n",
       "                                                text lang tweet_type  \\\n",
       "0  La ruta de easyJet entre Londres y Menorca tra...   es   original   \n",
       "1  @goody_tracy Here’s a list of some of @JonesDa...   en    retweet   \n",
       "2                                   @British_Airways  und      reply   \n",
       "3  Nice change by @AmericanAir. Bikes now pay sta...   en    retweet   \n",
       "4  BREAKING:-\\nKLM to fly 3x weekly btw @BLRAirpo...   en    retweet   \n",
       "\n",
       "           created_at  year  month  day  hour  text_len  \\\n",
       "0 2019-05-22 14:20:00  2019      5   22    14        19   \n",
       "1 2019-05-22 14:20:01  2019      5   22    14        38   \n",
       "2 2019-05-22 14:20:02  2019      5   22    14         1   \n",
       "3 2019-05-22 14:20:12  2019      5   22    14        23   \n",
       "4 2019-05-22 14:20:28  2019      5   22    14        21   \n",
       "\n",
       "                                          text_clean  text_clean_len  \n",
       "0  la ruta de easyjet entre londres y menorca tra...              17  \n",
       "1   goody_tracy heres a list of some of jonesday ...              38  \n",
       "2                                    british_airways               1  \n",
       "3  nice change by americanair. bikes now pay stan...              23  \n",
       "4  breaking - klm to fly 3x weekly btw blrairport...              22  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baf63d19",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efe4c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using framework PyTorch: 2.0.1+cu117\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "model = ORTModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", export=True, provider=\"CUDAExecutionProvider\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "onnx_classifier = pipeline(\"sentiment-analysis\",model=model,tokenizer=tokenizer, device = 0, max_length=512, truncation=True)                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88325c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_classifier(' easyjet harpercollinsch literacy_trust katiepiper_ face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting face_vomiting nauseated_face nauseated_face nauseated_face nauseated_face nauseated_face nauseated_face nauseated_face nauseated_face nauseated_face nauseated_face nauseated_face nauseated_face nauseated_face nauseated_face pouting_cat pouting_cat pouting_cat pouting_cat pouting_cat pouting_cat pouting_cat pouting_cat pouting_cat pouting_cat pouting_cat pouting_cat pouting_cat pouting_cat confounded_face confounded_face confounded_face confounded_face confounded_face confounded_face confounded_face confounded_face tired_face tired_face tired_face tired_face tired_face tired_face tired_face tired_face anxious_face_with_sweat anxious_face_with_sweat anxious_face_with_sweat anxious_face_with_sweat anxious_face_with_sweat anxious_face_with_sweat anxious_face_with_sweat anxious_face_with_sweat angry_face_with_horns angry_face_with_horns angry_face_with_horns angry_face_with_horns angry_face_with_horns angry_face_with_horns angry_face_with_horns angry_face_with_horns grimacing_face grimacing_face grimacing_face grimacing_face grimacing_face grimacing_face grimacing_face grimacing_face frowning_face frowning_face frowning_face frowning_face face_screaming_in_fear face_screaming_in_fear face_screaming_in_fear face_screaming_in_fear fearful_face fearful_face fearful_face fearful_face fearful_face fearful_face pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo pile_of_poo')[0]['label']\n",
    "# testing if truncation of tensors work for the longest tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fefc0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_dict = {}\n",
    "name_list = ['first', 'second', 'third', 'fourth','fifth', 'sixth', 'left']\n",
    "k = 0\n",
    "for i in range(0,7000001,1000001):\n",
    "    j = i + 1000000\n",
    "    if i == 6000006:\n",
    "        slice_dict[name_list[k]] = tweets.loc[i:]\n",
    "    else:\n",
    "        slice_dict[name_list[k]] = tweets.loc[i:j]    \n",
    "        k += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "224d7afb",
   "metadata": {},
   "source": [
    "# First mln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8e26be3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000001it [1:26:52, 191.87it/s]                       \n"
     ]
    }
   ],
   "source": [
    "# providing the sentiment analysis on the first milion tweets \n",
    "\n",
    "first_milion_ds = Dataset.from_pandas(slice_dict['first'])\n",
    "\n",
    "firts_milion_list = []\n",
    "for out in tqdm(onnx_classifier(KeyDataset(first_milion_ds, \"text_clean\"), batch_size=20)):\n",
    "\n",
    "    firts_milion_list.append(out['label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6b64c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_first_milion_ds = first_milion_ds.add_column(\"sentiment\", firts_milion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9ab74e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_first_milion_df = new_first_milion_ds.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b51b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert in the csv file\n",
    "\n",
    "new_first_milion_df.to_csv('first_mln_sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dd7ad71",
   "metadata": {},
   "source": [
    "# Second mln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1aea0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000001it [1:27:06, 191.33it/s]                       \n"
     ]
    }
   ],
   "source": [
    "# providing the sentiment analysis for the second milion \n",
    "\n",
    "second_milion_ds = Dataset.from_pandas(slice_dict['second'])\n",
    "\n",
    "second_milion_list = []\n",
    "for out in tqdm(onnx_classifier(KeyDataset(second_milion_ds, \"text_clean\"), batch_size=20)):\n",
    "\n",
    "    second_milion_list.append(out['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "077dad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_second_milion_ds = second_milion_ds.add_column(\"sentiment\", second_milion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a392982",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_second_milion_df = new_second_milion_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "193ad78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store in the csv file \n",
    "\n",
    "new_second_milion_df.to_csv('second_mln_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "57462ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all works correctly \n",
    "\n",
    "df = pd.read_csv('second_mln_sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90eb7cd0",
   "metadata": {},
   "source": [
    "# Third mln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5778b9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000001it [1:29:04, 187.12it/s]                       \n"
     ]
    }
   ],
   "source": [
    "third_milion_ds = Dataset.from_pandas(slice_dict['third'])\n",
    "\n",
    "third_milion_list = []\n",
    "for out in tqdm(onnx_classifier(KeyDataset(third_milion_ds, \"text_clean\"), batch_size=20)):\n",
    "\n",
    "    third_milion_list.append(out['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3084576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_third_milion_ds = third_milion_ds.add_column(\"sentiment\", third_milion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "58e93b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_third_milion_df = new_third_milion_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "545ba758",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_third_milion_df.to_csv('third_mln_sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ec6b375",
   "metadata": {},
   "source": [
    "# Fourth mln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c723056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "889781it [1:19:58, 173.37it/s]                        "
     ]
    }
   ],
   "source": [
    "fourth_milion_ds = Dataset.from_pandas(slice_dict['fourth'])\n",
    "\n",
    "fourth_milion_list = []\n",
    "for out in tqdm(onnx_classifier(KeyDataset(fourth_milion_ds, \"text_clean\"), batch_size=20)):\n",
    "\n",
    "    fourth_milion_list.append(out['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "959edb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fourth_milion_ds = fourth_milion_ds.add_column(\"sentiment\", fourth_milion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63e3e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fourth_milion_df = new_fourth_milion_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72802977",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fourth_milion_df.to_csv('fourth_mln_sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a5e05ac",
   "metadata": {},
   "source": [
    "# Fifth mln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f963ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251081it [22:41, 108.09it/s]                          "
     ]
    }
   ],
   "source": [
    "fifth_milion_ds = Dataset.from_pandas(slice_dict['fifth'])\n",
    "\n",
    "fifth_milion_list = []\n",
    "for out in tqdm(onnx_classifier(KeyDataset(fifth_milion_ds, \"text_clean\"), batch_size=20)):\n",
    "\n",
    "    fifth_milion_list.append(out['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61fdb45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fith_milion_ds = fith_milion_ds.add_column(\"sentiment\", fifth_milion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59a87c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fith_milion_df =  new_fourth_milion_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b941d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fith_milion_df.to_csv('fifth_mln_sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d5d899f",
   "metadata": {},
   "source": [
    "# Sixth mln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52f04b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixth_milion_ds = Dataset.from_pandas(slice_dict['sixth'])\n",
    "\n",
    "sixth_milion_list = []\n",
    "for out in tqdm(onnx_classifier(KeyDataset(sixth_milion_ds, \"text_clean\"), batch_size=20)):\n",
    "\n",
    "    sixth_milion_list.append(out['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98d45f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sixth_milion_ds = sixth_milion_ds.add_column(\"sentiment\", sixth_milion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10b9de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sixth_milion_df = new_sixth_milion_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "801cd58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sixth_milion_df.to_csv('sixth_mln_sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c864886d",
   "metadata": {},
   "source": [
    "# Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80c9f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94129it [08:11, 191.57it/s]                         \n"
     ]
    }
   ],
   "source": [
    "left_milion_ds = Dataset.from_pandas(slice_dict['left']) \n",
    "\n",
    "left_milion_list = []\n",
    "for out in tqdm(onnx_classifier(KeyDataset(left_milion_ds, \"text_clean\"), batch_size=20)):\n",
    "\n",
    "    left_milion_list.append(out['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "890b54e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_left_milion_ds = left_milion_ds.add_column(\"sentiment\", left_milion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4521788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_left_milion_df = new_left_milion_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f975e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_left_milion_df.to_csv('left_mln_sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f7c69b4",
   "metadata": {},
   "source": [
    "# Mrging csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46dab9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mln_1 = pd.read_csv('first_mln_sentiment')\n",
    "df_mln_2 = pd.read_csv('second_mln_sentiment')\n",
    "df_mln_3 = pd.read_csv('third_mln_sentiment')\n",
    "df_mln_4 = pd.read_csv('fourth_mln_sentiment')\n",
    "df_mln_5 = pd.read_csv('fifth_mln_sentiment')\n",
    "df_mln_6 = pd.read_csv('sixth_mln_sentiment')\n",
    "df_mln_7 = pd.read_csv('left_mln_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed68b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_cvs = [df_mln_1, df_mln_2, df_mln_3, df_mln_4, df_mln_5, df_mln_6, df_mln_7]\n",
    "\n",
    "for data in lst_cvs:\n",
    "    data.drop(\"Unnamed: 0\", inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0320dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(lst_cvs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c4f06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the scv with all sentiment analysis labels \n",
    "\n",
    "final_df.to_csv('df_sentiment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cc21744",
   "metadata": {},
   "source": [
    "# Testing effect of batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = tweets.loc[0:9999]\n",
    "\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39cfa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lst_st_mln =[]\n",
    "for text in tqdm(test_df['text_clean']):\n",
    "\n",
    "    out_lst_st_mln.append(get_score(text)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lst_st_mln =[]\n",
    "for out in tqdm(onnx_classifier(KeyDataset(test_ds, \"text_clean\"), batch_size=20)):\n",
    "\n",
    "    out_lst_st_mln.append(out['label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70850c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lst_st_mln =[]\n",
    "for out in tqdm(onnx_classifier(KeyDataset(test_ds, \"text_clean\"), batch_size=25)):\n",
    "\n",
    "    out_lst_st_mln.append(out['label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lst_st_mln =[]\n",
    "for out in tqdm(onnx_classifier(KeyDataset(test_ds, \"text_clean\"), batch_size=30)):\n",
    "\n",
    "    out_lst_st_mln.append(out['label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lst_st_mln =[]\n",
    "for out in tqdm(onnx_classifier(KeyDataset(test_ds, \"text_clean\"), batch_size=15)):\n",
    "\n",
    "    out_lst_st_mln.append(out['label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ds_6mln = Dataset.from_pandas(tweets)\n",
    "len(tweets_ds_6mln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c564ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for out in tqdm(onnx_classifier(KeyDataset(tweets_ds_6mln, \"text_clean\"), batch_size=20)):\n",
    "\n",
    "    final_list.append(out['label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e52691",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets_ds_6mln = tweets_ds_6mln.add_column(\"sentiment\", final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c954d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final =  pd.read_csv('df_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4be66ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>created_at</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_len</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1131172858951024641</td>\n",
       "      <td>393374091</td>\n",
       "      <td>1558527600406</td>\n",
       "      <td>La ruta de easyJet entre Londres y Menorca tra...</td>\n",
       "      <td>es</td>\n",
       "      <td>original</td>\n",
       "      <td>2019-05-22 14:20:00</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>la ruta de easyjet entre londres y menorca tra...</td>\n",
       "      <td>17</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1131172864147808257</td>\n",
       "      <td>3420691215</td>\n",
       "      <td>1558527601645</td>\n",
       "      <td>@goody_tracy Here’s a list of some of @JonesDa...</td>\n",
       "      <td>en</td>\n",
       "      <td>retweet</td>\n",
       "      <td>2019-05-22 14:20:01</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>goody_tracy heres a list of some of jonesday ...</td>\n",
       "      <td>38</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1131172867985485824</td>\n",
       "      <td>394376606</td>\n",
       "      <td>1558527602560</td>\n",
       "      <td>@British_Airways</td>\n",
       "      <td>und</td>\n",
       "      <td>reply</td>\n",
       "      <td>2019-05-22 14:20:02</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>british_airways</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1131172909463027720</td>\n",
       "      <td>36488556</td>\n",
       "      <td>1558527612449</td>\n",
       "      <td>Nice change by @AmericanAir. Bikes now pay sta...</td>\n",
       "      <td>en</td>\n",
       "      <td>retweet</td>\n",
       "      <td>2019-05-22 14:20:12</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>nice change by americanair. bikes now pay stan...</td>\n",
       "      <td>23</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1131172975682605058</td>\n",
       "      <td>14193348</td>\n",
       "      <td>1558527628237</td>\n",
       "      <td>BREAKING:-\\nKLM to fly 3x weekly btw @BLRAirpo...</td>\n",
       "      <td>en</td>\n",
       "      <td>retweet</td>\n",
       "      <td>2019-05-22 14:20:28</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>breaking - klm to fly 3x weekly btw blrairport...</td>\n",
       "      <td>22</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094130</th>\n",
       "      <td>6094130</td>\n",
       "      <td>1244696703690772485</td>\n",
       "      <td>278698748</td>\n",
       "      <td>1585593794163</td>\n",
       "      <td>Me parece a mí o el avión es más grande que el...</td>\n",
       "      <td>es</td>\n",
       "      <td>retweet</td>\n",
       "      <td>2020-03-30 20:43:14</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>me parece a m o el avin es ms grande que el si...</td>\n",
       "      <td>14</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094131</th>\n",
       "      <td>6094131</td>\n",
       "      <td>1244696708983984131</td>\n",
       "      <td>246520593</td>\n",
       "      <td>1585593795425</td>\n",
       "      <td>Today’s random pic of the day is the one of Vo...</td>\n",
       "      <td>en</td>\n",
       "      <td>original</td>\n",
       "      <td>2020-03-30 20:43:15</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>todays random pic of the day is the one of vol...</td>\n",
       "      <td>26</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094132</th>\n",
       "      <td>6094132</td>\n",
       "      <td>1244696710447800320</td>\n",
       "      <td>109284383</td>\n",
       "      <td>1585593795774</td>\n",
       "      <td>@spbverhagen @markduursma @StijnBz @KLM @AirFr...</td>\n",
       "      <td>nl</td>\n",
       "      <td>retweet</td>\n",
       "      <td>2020-03-30 20:43:15</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>spbverhagen markduursma stijnbz klm airfrance...</td>\n",
       "      <td>18</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094133</th>\n",
       "      <td>6094133</td>\n",
       "      <td>1244696713350217728</td>\n",
       "      <td>1223576386432126976</td>\n",
       "      <td>1585593796466</td>\n",
       "      <td>Tweede Kamer stemt over vliegtaks https://t.co...</td>\n",
       "      <td>nl</td>\n",
       "      <td>retweet</td>\n",
       "      <td>2020-03-30 20:43:16</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>44</td>\n",
       "      <td>tweede kamer stemt over vliegtaks via telegraa...</td>\n",
       "      <td>43</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094134</th>\n",
       "      <td>6094134</td>\n",
       "      <td>1244696713765564416</td>\n",
       "      <td>56784613</td>\n",
       "      <td>1585593796565</td>\n",
       "      <td>@easyJet My refund is being process since two ...</td>\n",
       "      <td>en</td>\n",
       "      <td>original</td>\n",
       "      <td>2020-03-30 20:43:16</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>easyjet my refund is being process since two ...</td>\n",
       "      <td>23</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6094135 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0             tweet_id              user_id   timestamp_ms  \\\n",
       "0                 0  1131172858951024641            393374091  1558527600406   \n",
       "1                 1  1131172864147808257           3420691215  1558527601645   \n",
       "2                 2  1131172867985485824            394376606  1558527602560   \n",
       "3                 3  1131172909463027720             36488556  1558527612449   \n",
       "4                 4  1131172975682605058             14193348  1558527628237   \n",
       "...             ...                  ...                  ...            ...   \n",
       "6094130     6094130  1244696703690772485            278698748  1585593794163   \n",
       "6094131     6094131  1244696708983984131            246520593  1585593795425   \n",
       "6094132     6094132  1244696710447800320            109284383  1585593795774   \n",
       "6094133     6094133  1244696713350217728  1223576386432126976  1585593796466   \n",
       "6094134     6094134  1244696713765564416             56784613  1585593796565   \n",
       "\n",
       "                                                      text lang tweet_type  \\\n",
       "0        La ruta de easyJet entre Londres y Menorca tra...   es   original   \n",
       "1        @goody_tracy Here’s a list of some of @JonesDa...   en    retweet   \n",
       "2                                         @British_Airways  und      reply   \n",
       "3        Nice change by @AmericanAir. Bikes now pay sta...   en    retweet   \n",
       "4        BREAKING:-\\nKLM to fly 3x weekly btw @BLRAirpo...   en    retweet   \n",
       "...                                                    ...  ...        ...   \n",
       "6094130  Me parece a mí o el avión es más grande que el...   es    retweet   \n",
       "6094131  Today’s random pic of the day is the one of Vo...   en   original   \n",
       "6094132  @spbverhagen @markduursma @StijnBz @KLM @AirFr...   nl    retweet   \n",
       "6094133  Tweede Kamer stemt over vliegtaks https://t.co...   nl    retweet   \n",
       "6094134  @easyJet My refund is being process since two ...   en   original   \n",
       "\n",
       "                  created_at  year  month  day  hour  text_len  \\\n",
       "0        2019-05-22 14:20:00  2019      5   22    14        19   \n",
       "1        2019-05-22 14:20:01  2019      5   22    14        38   \n",
       "2        2019-05-22 14:20:02  2019      5   22    14         1   \n",
       "3        2019-05-22 14:20:12  2019      5   22    14        23   \n",
       "4        2019-05-22 14:20:28  2019      5   22    14        21   \n",
       "...                      ...   ...    ...  ...   ...       ...   \n",
       "6094130  2020-03-30 20:43:14  2020      3   30    20        14   \n",
       "6094131  2020-03-30 20:43:15  2020      3   30    20        27   \n",
       "6094132  2020-03-30 20:43:15  2020      3   30    20        18   \n",
       "6094133  2020-03-30 20:43:16  2020      3   30    20        44   \n",
       "6094134  2020-03-30 20:43:16  2020      3   30    20        23   \n",
       "\n",
       "                                                text_clean  text_clean_len  \\\n",
       "0        la ruta de easyjet entre londres y menorca tra...              17   \n",
       "1         goody_tracy heres a list of some of jonesday ...              38   \n",
       "2                                          british_airways               1   \n",
       "3        nice change by americanair. bikes now pay stan...              23   \n",
       "4        breaking - klm to fly 3x weekly btw blrairport...              22   \n",
       "...                                                    ...             ...   \n",
       "6094130  me parece a m o el avin es ms grande que el si...              14   \n",
       "6094131  todays random pic of the day is the one of vol...              26   \n",
       "6094132   spbverhagen markduursma stijnbz klm airfrance...              18   \n",
       "6094133  tweede kamer stemt over vliegtaks via telegraa...              43   \n",
       "6094134   easyjet my refund is being process since two ...              23   \n",
       "\n",
       "        sentiment  \n",
       "0         neutral  \n",
       "1         neutral  \n",
       "2         neutral  \n",
       "3        positive  \n",
       "4         neutral  \n",
       "...           ...  \n",
       "6094130  negative  \n",
       "6094131  positive  \n",
       "6094132  negative  \n",
       "6094133  negative  \n",
       "6094134  negative  \n",
       "\n",
       "[6094135 rows x 16 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
